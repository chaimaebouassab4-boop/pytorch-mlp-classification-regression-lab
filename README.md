# pytorch-mlp-classification-regression-lab
"PyTorch lab project implementing DNN/MLP models for classification and regression with regularization and performance analysis."

# ğŸ“˜ **Lab 1 â€” Deep Learning with PyTorch**

### **DNN/MLP for Regression & Multi-Class Classification**

---

## ğŸ¯ **Objective**

The main purpose of this lab is to become familiar with the **PyTorch** library by implementing **Regression** and **Multi-Class Classification** tasks using **Deep Neural Network (DNN/MLP) architectures**.
You will explore data, build models, tune hyperparameters, apply regularization, and evaluate performance.

---

## ğŸ§° **Tools Used**

* **Python**
* **PyTorch**
* **Scikit-Learn**
* **Pandas / NumPy**
* **Matplotlib / Seaborn**
* **Google Colab or Kaggle**
* **Git / GitHub**

---

# ğŸ§© **Part 1 â€” Regression Task**

### ğŸ“Œ **Dataset**

â¡ï¸ NYSE Dataset:
[https://www.kaggle.com/datasets/dgawlik/nyse](https://www.kaggle.com/datasets/dgawlik/nyse)

---

### âœ”ï¸ **1. Exploratory Data Analysis (EDA)**

* Descriptive statistics
* Missing values
* Correlation matrix
* Visualizations (histograms, boxplots, heatmaps, etc.)

---

### âœ”ï¸ **2. Build a DNN/MLP for Regression (PyTorch)**

* Input/hidden/output layers
* Activation functions
* Optimizer
* Loss function (MSELoss)
* Training loop

---

### âœ”ï¸ **3. Hyperparameter Tuning (GridSearch â€“ sklearn)**

Parameters explored:

* Learning rate
* Optimizer (SGD, Adam, RMSPropâ€¦)
* Number of layers & neurons
* Batch size
* Epochs

---

### âœ”ï¸ **4. Plot & Interpret Training Curves**

* **Loss vs Epochs (Train/Test)**
* **Accuracy vs Epochs (Train/Test)** *(if accuracy applies)*

Explain:

* Underfitting / overfitting
* Convergence
* Stability

---

### âœ”ï¸ **5. Apply Regularization Techniques**

Regularization methods tested:

* **Dropout**
* **Weight decay (L2)**
* **Batch Normalization**
* **Early Stopping**

Compare results with the first (non-regularized) model.


# ğŸ§© **Part 2 â€” Multi-Class Classification Task**

### ğŸ“Œ **Dataset**

â¡ï¸ Predictive Maintenance Dataset:
[https://www.kaggle.com/datasets/shivamb/machine-predictive-maintenance-classification](https://www.kaggle.com/datasets/shivamb/machine-predictive-maintenance-classification)

---

### âœ”ï¸ **1. Data Cleaning & Pre-Processing**

* Handle missing values
* Encoding categorical features
* Standardization / Normalization

---

### âœ”ï¸ **2. Exploratory Data Analysis (EDA)**

* Class distribution
* Correlation analysis
* Feature distributions
* Outliers

---

### âœ”ï¸ **3. Apply Data Augmentation**

Since the dataset is imbalanced:

* Oversampling (SMOTE)
* Undersampling
* Synthetic data generation

---

### âœ”ï¸ **4. Build DNN/MLP for Multi-Class Classification**

* CrossEntropyLoss
* One-hot labels (if needed)
* Softmax output layer

---

### âœ”ï¸ **5. Hyperparameter Tuning (GridSearch â€“ sklearn)**

Tune:

* LR
* Optimizer
* Number of neurons
* Number of layers
* Batch size
* Epoch count

---

### âœ”ï¸ **6. Plot & Interpret Training Curves**

* Loss vs Epochs (Train/Test)
* Accuracy vs Epochs (Train/Test)

---

### âœ”ï¸ **7. Compute Performance Metrics**

For both **train** and **test** sets:

* Accuracy
* Precision
* Recall (Sensitivity)
* F1-Score
* Confusion Matrix

---

### âœ”ï¸ **8. Apply Regularization Techniques**

Compare model performance **before/after**:

* Dropout
* Weight decay (L2)
* BatchNorm
* Early stopping

---

# ğŸ“ **Conclusion**

### ğŸ“Œ **Key Learnings Synthesis**

* Understanding PyTorch workflow
* Building DNN/MLP architectures
* Applying data pre-processing
* Hyperparameter tuning with GridSearch
* Interpreting training curves
* Using mathematical metrics (Precision, Recall, F1, Cross-Entropy, MSE)
* Evaluating overfitting and using regularization

### ğŸ“Œ **Comparative Analysis**

* Performance improvement after tuning
* Effectiveness of regularization
* Comparison between regression and classification behaviors
* Impact of balanced vs imbalanced data on model learning

---

# ğŸ“¦ **Repository Structure**

```
ğŸ“ pytorch-mlp-classification-regression-lab
â”‚â”€â”€ ğŸ“ regression
â”‚   â”œâ”€â”€ eda.ipynb
â”‚   â”œâ”€â”€ regression_model.py
â”‚   â”œâ”€â”€ gridsearch.ipynb
â”‚   â”œâ”€â”€ results/
â”‚
â”‚â”€â”€ ğŸ“ classification
â”‚   â”œâ”€â”€ preprocessing.ipynb
â”‚   â”œâ”€â”€ classification_model.py
â”‚   â”œâ”€â”€ regularization_experiments.ipynb
â”‚   â”œâ”€â”€ results/
â”‚
â”‚â”€â”€ README.md
â”‚â”€â”€ requirements.txt
```

---

# ğŸ‘¨â€ğŸ« **Instructor**

**Pr. ELAACHAK LOTFI**
*MBD Master â€” Deep Learning*
*UniversitÃ© Abdelmalek Essaadi, FST Tanger*

